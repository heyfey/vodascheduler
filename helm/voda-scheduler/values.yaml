# Default values for voda-scheduler.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

tolerations:
  - key: "vodascheduler/hostname"
    operator: "Exists"
    effect: "NoExecute"

rabbitmq:
  ## Because several features (e.g. quorum queues, client tracking in MQTT) require 
  ## a consensus between cluster members, odd numbers of cluster nodes are highly recommended: 1, 3, 5, 7 and so on.
  ## ref: https://www.rabbitmq.com/clustering.html#node-count
  replicaCount: 3

  ## You may want to override this
  username: "guest"
  password: "guest"

  storageClass: voda-rabbitmq-nfs-client

  ## Pod scheduling preferences (by default keep pods within a release on separate nodes).
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## By default we encougrage RabbitMQ pods on separate nodes
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 90
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - rabbitmq
          topologyKey: kubernetes.io/hostname


  resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

mongo:
  image:
    repository: mongo
    tag: "5"
  
  resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  persistentVolume:
    storageClass: voda-mongodb-nfs-client
    ## Setting the storageClass attribute to "-" disables dynamic
    ## provisioning of Persistent Volumes and creates Persistent Volumes 
    ## using specified type below
    # storageClass: "-"

    type: nfs

    ## mongo data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteMany

    ## mongo data Persistent Volume size
    ##
    size: 5Gi

    ## If using NFS for PV
    ## TODO: have the following set    
    nfs:
      server: 192.168.20.100
      path: "/volume1/homes/heyfey/voda/mongodb/v1beta1"
      
metricscollector:
  image:
    repository: heyfey/voda-metrics-collector
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "0.2.2"
    # tag: "test"

  ## schedule of the CronJob, every minute by default
  ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#cron-schedule-syntax
  schedule: "*/1 * * * *"

  resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi
  
  persistentVolume:
    storageClass: voda-metrics-nfs-client
    ## Setting the storageClass attribute to "-" disables dynamic
    ## provisioning of Persistent Volumes and creates Persistent Volumes 
    ## using specified type below
    # storageClass: "-"
    
    type: nfs

    ## metricscollector data Persistent Volume access modes
    ## Must match those of existing PV or dynamic provisioner
    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    accessModes:
      - ReadWriteMany

    ## metricscollector data Persistent Volume size
    ##
    size: 5Gi

    ## If using NFS for PV
    ## TODO: have the following set  
    nfs:
      server: 192.168.20.100
      path: "/volume1/homes/heyfey/voda/metrics"

trainingservice:
  replicaCount: 2
  image:
    repository: heyfey/voda-training-service
    # Overrides the image tag whose default is the chart appVersion.
    tag: "0.2.2"
    # tag: "test"

    # pullPolicy: IfNotPresent
    pullPolicy: Always


  service:
    type: ClusterIP
    # fixed ip should be same as in config/config.go
    clusterIP: 10.100.86.93
    # port number should be same as in config/config.go
    port: 55587
    targetPort: 55587

  ## Pod scheduling preferences (by default keep pods within a release on separate nodes).
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## By default we encougrage pods on separate nodes
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 90
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - training-service
          topologyKey: kubernetes.io/hostname

  resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

resourceallocator:
  replicaCount: 2
  image:
    repository: heyfey/voda-resource-allocator
    # Overrides the image tag whose default is the chart appVersion.
    tag: "0.2.2"
    # tag: "test"

    # pullPolicy: IfNotPresent
    pullPolicy: Always

  service:
    type: ClusterIP
    clusterIP: 10.100.86.94
    port: 55589

  ## Pod scheduling preferences (by default keep pods within a release on separate nodes).
  ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## By default we encougrage pods on separate nodes
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 90
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                - resource-allocator
          topologyKey: kubernetes.io/hostname

  resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

scheduler:
  image:
    repository: heyfey/voda-scheduler
    # Overrides the image tag whose default is the chart appVersion.
    tag: "0.2.2"
    # tag: "test"

    # pullPolicy: IfNotPresent
    pullPolicy: Always

  resumeEnabled: true

  ## Scheduling algorithm of the scheduler.
  ## Which options are supported depends on resource allocator.
  ##
  ## Elastic algorithm:
  algorithm: ElasticFIFO
  # algorithm: ElasticSRJF
  # algorithm: ElasticTiresias
  # algorithm: FfDLOptimizer
  # algorithm: AFS-L

  ## Non-elastic algorithm:
  # algorithm: FIFO
  # algorithm: SRJF
  # algorithm: Tiresias


  # Debug log level for the scheduler. 5 is the most comprehensive.
  # debugLevel: 1
  # debugLevel: 2
  # debugLevel: 3
  # debugLevel: 4
  # debugLevel: 5

  # placementDisabled disables placement management of the scheduler, including
  # topology-aware scheduling and worker migration.
  # placementDisabled: true

  # If configmapOpt is true, the scheduler will update dummy annotation of
  # launcher pod every time when its mpijob is scaled. This is to intentionally
  # trigger a pod sync that will sync the config map, so Horovod can get the
  # updated host list with minimun delay.
  configmapOpt: true
  # configmapOpt: false

  resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  service:
    type: ClusterIP
    port: 55588

